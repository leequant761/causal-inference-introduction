{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"6 - Estimation.assets/image-20220818151351911.png\" alt=\"image-20220818151351911\">\n",
    "\n",
    "# 0. Notation\n",
    "\n",
    "* CATEs : Conditional average treatment effects\n",
    "  * $\\tau(x) := \\mathbb E [Y(1) - Y(0) \\mid X=x]$\n",
    "* Positivity & NUC\n",
    "  * $\\tau := \\mathbb E_W[\\mathbb E_Y [Y\\mid T=1, W] - \\mathbb E_Y [Y\\mid T=0, W]]$\n",
    "    * $W$ is a sufficient adjustment set\n",
    "  * $\\tau(x) := \\mathbb E_W[\\mathbb E_Y [Y\\mid T=1, X=x, W] - \\mathbb E_Y [Y\\mid T=0, X=x, W]]$\n",
    "    * $W \\cup X$ is a sufficient adjustment set\n",
    "\n",
    "# 1. Conditional Outcome Modeling\n",
    "\n",
    "* $\\tau := \\mu(1, W) - \\mu(0, W)$\n",
    "* Conditional Outcome Model (COM)\n",
    "  * $\\hat \\mu$\n",
    "    * linear model\n",
    "    * neural network\n",
    "    * etc.\n",
    "* COM estimator for the\n",
    "  * ATE\n",
    "    * $\\hat \\tau := \\frac{1}{n} \\sum_i (\\hat \\mu (1, w_i) - \\hat \\mu (0, w_i) )$\n",
    "  * CATE\n",
    "    * $\\hat \\tau(x) := \\frac{1}{n_x} \\sum_{i: x_i= x} (\\hat \\mu (1, w_i, x) - \\hat \\mu (0, w_i, x) )$\n",
    "* COM estimation은 많은 이름으로 불림\n",
    "  * G-computation estimator\n",
    "  * Parametric G-formula\n",
    "  * S-learner\n",
    "  * etc.\n",
    "\n",
    "## 1.1 Problem with COM estimation in high dims\n",
    "\n",
    "<img src=\"6 - Estimation.assets/image-20220821161215239.png\" alt=\"image-20220821161215239\">\n",
    "\n",
    "* $T$가 1차원이라 $T$없이 $W$만으로 예측하는 머신이 나올 수 있음\n",
    "* 그 경우, $\\hat \\tau \\approx 0$이 나오고 estimator가 biased임을 암시\n",
    "\n",
    "## 1.2 Grouped COM (GCOM) estimation\n",
    "\n",
    "<img src=\"6 - Estimation.assets/image-20220821162102383.png\" alt=\"image-20220821162102383\">\n",
    "\n",
    "* GCOM estimator for the\n",
    "  * ATE\n",
    "    * $\\hat \\tau := \\frac{1}{n} \\sum_i (\\hat \\mu_1 (w_i) - \\hat \\mu_0 (w_i) )$\n",
    "  * CATE\n",
    "    * $\\hat \\tau(x) := \\frac{1}{n_x} \\sum_{i: x_i= x} (\\hat \\mu_1 (w_i, x) - \\hat \\mu_0 (w_i, x) )$\n",
    "\n",
    "* 두 개의 모델을 따로 학습함\n",
    "* 데이터 효율이 떨어지고 높은 분산을 야기함\n",
    "\n",
    "## 1.3. Increasing Data Efficiency\n",
    "\n",
    "### 1.3.1 TARNet\n",
    "\n",
    "<img src=\"6 - Estimation.assets/image-20220821165410787.png\" alt=\"image-20220821165410787\">\n",
    "\n",
    "* 모든 데이터로 Representation Learning은 하지만,\n",
    "* Predictor 학습은 따로한 후에\n",
    "* COM estimator로 활용\n",
    "  * ATE\n",
    "    * $\\hat \\tau = \\frac{1}{n} \\sum_i (\\hat \\mu (1, w_i) - \\hat \\mu (0, w_i) )$\n",
    "  * CATE\n",
    "    * $\\hat \\tau(x) = \\frac{1}{n_x} \\sum_{i: x_i= x} (\\hat \\mu (1, w_i, x) - \\hat \\mu (0, w_i, x) )$\n",
    "* 하지만 이 역시도, predictor 학습을 따로 하기 때문에 아쉬움\n",
    "\n",
    "### 1.3.2 X-Learner\n",
    "\n",
    "* STEP1\n",
    "  * Estimate $\\hat \\mu_1(x), \\hat \\mu_0(x)$\n",
    "* STEP2\n",
    "  * Impute ITE\n",
    "    * Treatment group\n",
    "      * $\\hat \\tau_{1, i} = Y_i(1) - \\hat \\mu_0 (x_i)$\n",
    "    * Control group\n",
    "      * $\\hat \\tau_{0, i} = \\hat \\mu_1 (x_i) - Y_i(0)$\n",
    "  * Train by using imputed ITEs so that\n",
    "    * Treatment group\n",
    "      * $\\hat \\tau_{1}(x_i) : x_i \\rightarrow \\hat \\tau_{1, i}$\n",
    "    * Control group\n",
    "      * $\\hat \\tau_{0}(x_i) : x_i \\rightarrow \\hat \\tau_{0, i}$\n",
    "* STEP3\n",
    "  *  $\\hat \\tau(x) = g(x) \\hat\\tau_0(x) + (1- g(x)) \\hat \\tau_1(x)$\n",
    "  *  where $g(x)$ is some weighting function\n",
    "     * propensity score\n",
    "     * etc.\n",
    "\n",
    "# 2. Propensity Scores and IPW\n",
    "\n",
    "* Sufficient adjustment set $W$가 고차원이면?\n",
    "\n",
    "## 2.1 Propensity Score\n",
    "\n",
    "* $e(W) = P(T=1 \\mid W)$를 모델링하는 것이 낫다.\n",
    "  * logistic regression\n",
    "  * etc.\n",
    "\n",
    "### Propensity Score Theorem\n",
    "\n",
    "<img src=\"6 - Estimation.assets/image-20220822015012588.png\" alt=\"image-20220822015012588\">\n",
    "\n",
    "* Given **positivity**,\n",
    "  * $(Y(1), Y(0)) \\perp T \\mid W$ implies $(Y(1), Y(0)) \\perp T \\mid e(W)$\n",
    "\n",
    "## 2.2 Inverse Probability Weighting (IPW)\n",
    "\n",
    "<img src=\"6 - Estimation.assets/image-20220822015547077.png\" alt=\"image-20220822015547077\">\n",
    "\n",
    "* Population을 reweight하여 $W$와 $T$의 dependency를 제거\n",
    "* $\\mathbb E[Y(t)] = \\mathbb E[\\frac{\\mathbb 1 (T=t) Y}{P(t \\mid W)}]$\n",
    "* $\\tau = \\mathbb E[Y(1) - Y(0)] = \\mathbb E[\\frac{\\mathbb 1 (T=1) Y}{e(W)}] - \\mathbb E[\\frac{\\mathbb 1 (T=0) Y}{1-e(W)}]$\n",
    "* $\\hat \\tau = \\frac{1}{n} \\sum_{i: t_i = 1} \\frac{y_i}{\\hat e(w_i)} - \\frac{1}{n} \\sum_{i: t_i = 0} \\frac{y_i}{1-\\hat e(w_i)}$\n",
    "\n",
    " ### IPW \"C\"ATE estimation\n",
    "\n",
    "* See *Estimating Conditional Average Treatment Effects*\n",
    "\n",
    "# 3. Other Methods\n",
    "\n",
    "* Doubly robust methods\n",
    "  * COM과 Propensity score를 동시에 모델링\n",
    "  * 이론적으로 COM이나 IPW보다 더 빨리 수렴\n",
    "  * $\\frac{1}{n} \\sum_i(\\hat \\mu(1, \\hat e(w_i)) - \\hat \\mu (0, 1-\\hat e(w_i)))$\n",
    "* Double machine learning"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
